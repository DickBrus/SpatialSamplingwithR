# Balanced and well-spread sampling {#BalancedSpreaded}

In this chapter two sampling designs are described and illustrated that are related but also fundamentally different. The similarities and difference are shortly described, but will become more clear in following sections. Roughly speaking, a balanced sample is a sample of which the sample mean of one or more covariates is equal to the population means of these covariates. When the covariates are linearly related to the study variable this may yield a more accurate estimate of the population mean or total.

A well-spread sample\index{Well-spread sample} is a sample with a large range of values for the covariates, from small to large values, but also intermediate values. In more technical terms: the sampling units are well-spread along the axes spanned by the covariates. If the spatial coordinates are used as covariates (spreading variables), this results in samples that are well-spread in geographical space. Such samples are commonly referred to as spatially balanced samples, which is somewhat confusing, as the geographical spreading is not implemented through balancing on the geographical coordinates. On the other hand, the averages of the spatial coordinates of a sample well-spread in geographical space will be close to the population means of the coordinates, and therefore will be approximately balanced on the spatial coordinates [@Grafstrom2014]. The reverse is not true: with balanced sampling\index{Balanced sampling} the spreading of the sampling units in the space spanned by the balancing variables can be poor. A sample with all values of a covariate used in balancing near the population mean of that variable has a poor spreading along the covariate axis, but can still be perfectly balanced.  

## Balanced sampling {#Balanced}

Balanced sampling is a sampling method that exploits one or more quantitative covariates that are related to the study variable. The idea behind balanced sampling is that, if we know the mean of the covariates, then the sampling efficiency can be increased by selecting a sample whose averages of the covariates must be equal to the population means of the covariates.

The simulated population shown in Figure \@ref(fig:simpleexample) shows a linear trend from West to East, and also a trend from South to North, but we will ignore this South to North trend for the moment. In other words, the simulated study variable $z$ is correlated with the covariate Easting. To estimate the population mean of the simulated study variable, intuitively it is attractive to select a sample with an average of the Easting coordinate that is equal to the population mean of Easting (which is 10). Figure \@ref(fig:simpleexample)(a) shows such a sample; we say that the sample is `balanced' on the covariate Easting.

```{r simpleexample, echo=FALSE, out.width='100%', fig.cap = "Sample balanced on Easting (E) and on Easting and Northing (E and N)."}
library(sampling)
library(gstat)
#define residual variogram for simulation
vgmodel <- vgm(model="Exp", psill=10, range=4, nugget=0)

#define discretisation grid
x <- 1:20 - 0.5
y <- x
grid <- expand.grid(x,y)
names(grid) <- c("x1","x2")
distx <- outer(grid$x1, grid$x1, FUN="-")
disty <- outer(grid$x2, grid$x2, FUN="-")
dist <- sqrt(distx^2+disty^2)

#compute matrix with covariances
C <- variogramLine(vgmodel, dist_vector=dist, covariance=TRUE)

#now simulate values for grid by Cholesky decomposition
Upper <- chol(C)

set.seed(31415)
G <- rnorm(n=nrow(grid),0,1) #simulate random numbers from standard normal distribution

#trend coefficient in x-direction
b1 <- 2
b2 <- 1
grid$z <- crossprod(Upper,G) + b1*grid$x1 + b2*grid$x2

#compute population size
N <- nrow(grid)

#set sample size)
n <- 4

#define matrix with covariate for balancing; first column of matrix must be filled with ones
X <- cbind(rep(1,times=N), grid$x1)

#compute inclusion probabilities; use equal probabilities
pi <- rep(n/N,times=N)

nsam <- 100
mx_pop <- mean(grid$x1)
set.seed(31415)
repeat {
    sample_ind <- samplecube(X=X, pik=pi, comment=FALSE, method=1)
    mysample <- grid[sample_ind==1,]
    mx_sample <- mean(mysample$x1)
    if (mx_sample == mx_pop) {break}    
}

#now select a sample balanced on Easting and Northing
X <- cbind(rep(1,times=N),grid$x1,grid$x2)

mx1_pop <- mean(grid$x1)
mx2_pop <- mean(grid$x2)
set.seed(314)
repeat {
  sample_ind <- samplecube(X=X, pik=pi, comment=FALSE, method=1)
  mysample2 <- grid[sample_ind==1,]
  mx1_sample <- mean(mysample2$x1)
  mx2_sample <- mean(mysample2$x2)
  if (mx1_sample == mx1_pop & mx2_sample == mx2_pop) {break}    
}

mysamples <- rbind(mysample,mysample2)
mysamples$samid <- rep(c("E","E and N"), each=4)

ggplot(data=grid) +
  geom_tile(mapping=aes(x=x1, y=x2, fill=z)) +
  geom_tile(data=mysamples, mapping=aes(x=x1, y=x2), colour="white", size=0.8, width=1, height=1, fill=NA)+
  scale_fill_continuous(name="x",type= "viridis") +
  scale_x_continuous(name="Easting") +
  scale_y_continuous(name="Northing") +
  facet_wrap(~ samid, ncol=2, nrow=1) +
  coord_fixed()
```

### Balanced sample versus balanced sampling design

We must distinguish a balanced *sample* from a balanced sampling *design*. A sampling design is balanced on a covariate $x$ when *all possible* samples that can be generated by the design are balanced on $x$. So, simple random sampling is not a balanced sampling design, because for many simple random samples the sample mean of $x$ is not equal to the population mean of $x$. Only the *expectation* of the sample mean of $x$, i.e. the mean of the sample mean over an infinite number of simple random samples, equals the population mean of $x$.

Figure \@ref(fig:scatterplotsqerror) shows for one thousand simple random samples the squared error of the estimated population mean of the study variable $z$ against the difference between the sample mean of $x$ and the population mean of $x$.

```{r scatterplotsqerror, echo=FALSE, fig.cap="Squared error in estimated mean of $z$ against difference between population and sample mean of a covariate."}
nsam <- 1000
n <- 4
mz_sample <- mx_sample <- numeric(length=nsam)
set.seed(31415)
for (i in 1:nsam) {
  #select simple random sample from the data
  sample_ind <- sample.int(nrow(grid), size=n)
  mz_sample[i] <- mean(grid$z[sample_ind])
  mx_sample[i] <- mean(grid$x1[sample_ind])
}

sqerror <- (mz_sample - mean(grid$z))^2
devx <- mx_sample - mean(grid$x1)
df <- data.frame(sqerror, devx)
ggplot(data=df) +
  geom_point(mapping=aes(y=sqerror ,x=devx), size=2) +
  scale_x_continuous(name="Sample mean x - population mean x", limits=c(-9,9)) +
  scale_y_continuous(name="Squared error")
```

Clearly, the larger the absolute value of the difference, the larger on average the squared error. So to obtain an accurate estimate of the population mean of $z$, we better select samples with a difference close to 0.

Sampling designs can also be balanced on multiple covariates. Figure \@ref(fig:simpleexample)(b) shows a sample balanced on both Easting and Northing. Using Easting as a balancing variable reduced the sampling variance of the estimator of the mean substantially, see Table \@ref(tab:tablebalanced). Using Northing as a second balancing variable further reduced the sampling variance.  

```{r tablebalanced, echo=FALSE}
tbl <- data.frame(x=c("SI","Balanced","Balanced"),y=c("-","Easting","Easting+Northing"),z=c(39.7,14.4,9.77))

knitr::kable(
  tbl, caption = 'Sampling variances of estimated mean for simple random sampling and balanced sampling of four units.',
  col.names=c("Sampling design","Balancing variables","Sampling variance"),
  booktabs = TRUE
) %>%
  kable_classic()
```

### Unequal inclusion probabilities

Until now we assumed that the inclusion probabilities of the population units are equal, but this is not a requirement for balanced sampling designs. A more general definition is: a sampling design is balanced on variable $x$ when for all samples generated by the design the $\pi$ estimator of the population total of $x$ equals the population total of $x$:  

\begin{equation}
\sum_{k \in \mathcal{S}} \frac{x_k}{\pi_k}= \sum_{k=1}^{N} x_k \;,
(\#eq:generaldefinitionbalanced)
\end{equation}

with $\pi_k$ the inclusion probability of unit $k$, $x_k$ the covariate of unit $k$, and $N$ the total number of units in the population.

Similar to the regression estimator\index{Regression estimator} (Section \@ref(RegressionEstimator)), balanced sampling exploits the linear relation between the study variable and one or more covariates. In the regression estimator this is done at the estimation stage. Balanced sampling does so at the sampling stage. For a single covariate the regression estimator of the population total equals (see Equation \@ref(eq:SimpleRegressionEstimatorSI))

\begin{equation}
\hat{t}_{\mathrm{regr}}(z) = \hat{t}_{\pi}(z) + \hat{b}\left(t(x) - \hat{t}_{\pi}(x)\right) \;,
(\#eq:RegressionEstimatoranydesign)
\end{equation}

with $\hat{t}_{\pi}(z)$ and $\hat{t}_{\pi}(x)$ the $\pi$ estimators of the population total of the study variable $z$ and the covariate $x$, respectively, $t(x)$ the population total of the covariate, and $\hat{b}$ the estimated slope parameter (see hereafter). With a perfectly balanced sample the adjustment term in the regression estimator (the second term) equals zero.

Balanced samples can be selected with the cube algorithm\index{Cube algorithm for balanced sampling} of @Deville2004. The population total can be estimated by the $\pi$ estimator:

\begin{equation}
\hat{t}(z) = \sum_{k \in \mathcal{S}} \frac{z_k}{\pi_k}.
(\#eq:HTMeanBalanced)
\end{equation}

So with equal inclusion probabilities, equal to $n/N$, the population total is estimated by the sample mean of the study variable multiplied by the number of population units $N$. The population mean is estimated by dividing the estimated population total by $N$.

The approximate variance of the $\pi$ estimator of the population mean can be estimated by (@Deville2005, @Grafstrom2013) 

\begin{equation}
\widehat{V}(\hat{\bar{z}}) = \frac{1}{N^2}\frac{n}{n-p} \sum_{k \in \mathcal{S}} c_k \left(\frac{\epsilon_k}{\pi_k}\right)^2 \;,
(\#eq:approxvarianceBalanced)
\end{equation}

with $p$ the number of balancing variables, $c_k$ a weight for unit $k$ (see hereafter), and $\epsilon_k$ the residual of unit $k$ given by

\begin{equation}
\epsilon_k = z_k - \mathbf{x}_k^{\text{T}}\hat{\mathbf{b}} \;,
(\#eq:residualsBalanced)
\end{equation}

with $\mathbf{x}_k$ a vector of length $p$ with the balancing variables for unit $k$, and $\hat{\mathbf{b}}$ the estimated population regression coefficients\index{Population regression coefficient}, given by

\begin{equation}
\hat{\mathbf{b}} = \left(\sum_{k \in \mathcal{S}} c_k \frac{\mathbf{x}_k}{\pi_k} \frac{\mathbf{x}_k}{\pi_k}^{\text{T}} \right)^{-1} \sum_{k \in \mathcal{S}} c_k \frac{\mathbf{x}_k}{\pi_k} \frac{z_k}{\pi_k} \;.
(\#eq:betasbalanced)
\end{equation}

Working this out for balanced sampling without replacement with equal inclusion probabilities, $\pi_k = n/N, k = 1, \dots , N$, yields

\begin{equation}
\widehat{V}(\hat{\bar{z}}) = \frac{1}{n(n-p)} \sum_{k \in \mathcal{S}} c_k \epsilon_k^2 \;.
(\#eq:approxvarianceBalancedSI)
\end{equation}

@Deville2005 give several formulas for computing the weights $c_k$, one of which is $c_k = (1-\pi_k)$.

Balanced sampling is now illustrated with the aboveground biomass (AGB) data of Eastern Amazonia, see Figure \@ref(fig:mapsAmazonia). Log-transformed short-wave infrared (lnSWIR2) is used as a balancing variable. The `samplecube` function of the **sampling** package [@Tille2016] implements the cube algorithm. Argument `X` of this function specifies the matrix of ancillary variables on which the sample must be balanced. The first column of this matrix must be filled with ones, so that the sample size is fixed. Equal inclusion probabilities are used, i.e. for all population units the inclusion probability equals $n/N$.

```{r}
load("data/Amazonia_5km.RData")
gridAmazonia$lnSWIR2 <- log(gridAmazonia$SWIR2)
library(sampling)
N <- nrow(gridAmazonia)
n <- 100
X <- cbind(rep(1,times=N), gridAmazonia$lnSWIR2)
pi <- rep(n/N,times=N)
sample_ind <- samplecube(X=X, pik=pi, comment=FALSE, method=1)
eps <- 1e-6
units <- which(sample_ind>(1-eps))
mysample <- gridAmazonia[units,]
```

The population mean can be estimated by the sample mean.

```{r}
mz_sample <- mean(mysample$AGB)
```

To estimate the variance a function is defined for estimating the population regression coefficients. 

```{r}
estimate_b <- function(z,X,c) {
  cXX <- matrix(nrow=ncol(X), ncol=ncol(X), data=0)
  cXz <- matrix(nrow=1, ncol=ncol(X), data=0)
  for (i in 1:length(z)) {
    x <- X[i,]
    cXX_i <- c[i]*(x %*% t(x))
    cXX <- cXX+cXX_i
    cXz_i <- c[i]*t(x)*z[i]
    cXz <- cXz+cXz_i
  }
  b <- solve(cXX) %*% t(cXz)
  b
}
```

The next code chunk shows how the variance of the $\pi$ estimator of the population mean can be estimated.

```{r}
pi <- rep(n/N,n)
c <- (1-pi)
b <- estimate_b(z=mysample$AGB/pi, X=X[units,]/pi, c=c)
zpred <- X%*%b
e <- mysample$AGB-zpred[units]
v_tz <- n/(n-ncol(X))*sum(c*(e/pi)^2)
v_mz <- v_tz/N^2
```

Figure \@ref(fig:BalancedSampleAmazonia) shows the result. The sample mean of AGB equals `r as.character(round(mz_sample,1))`. The population mean of AGB equals `r as.character(round(mean(gridAmazonia$AGB),1))`. Note the spatial clustering of some units. The standard error of the estimated mean equals `r as.character(round(sqrt(v_mz),1))`.

```{r BalancedSampleAmazonia, echo=FALSE, out.width='100%', fig.cap="Balanced sample from Eastern Amazonia (Brazil), balanced on covariate lnSWIR2."}
ggplot(data=gridAmazonia) +
  geom_raster(mapping=aes(x=x1/1000, y=x2/1000, fill=lnSWIR2)) +
  geom_point(data=mysample, mapping=aes(x=x1/1000, y=x2/1000), size=2)+
  scale_fill_continuous(name="lnSWIR2", type="viridis") +
  scale_x_continuous(name="Easting (km)") +
  scale_y_continuous(name="Northing (km)") +
  coord_fixed()
```

Figure \@ref(fig:SamplingDistributionBalanced) shows the sampling distributions of the $\pi$ estimator of the mean of AGB with balanced sampling and simple random sampling, obtained by repeating the random sampling with both designs and estimation 1,000 times.

```{r, echo=FALSE, eval=FALSE}
mz <- v_mz <- mx_sample <- mz_SI <- numeric(length=1000)

pi <- rep(n/N, n)
c <- (1-pi)

set.seed(314)
for (i in 1:1000) {
  sample_ind <- samplecube(X=X, pik=rep(n/N,N), comment=FALSE, method=1)
  units <- which(sample_ind>(1-eps))
  mysample <- gridAmazonia[units,]

  mz[i] <- mean(mysample$AGB)
  
  b <- estimate_b(z=mysample$AGB/pi, X=X[units,]/pi, c=c)
  zpred <- X%*%b
  e <- mysample$AGB-zpred[units]
  v_tz <- n/(n-ncol(X))*sum(c*(e/pi)^2)
  v_mz[i] <- v_tz/N^2

  mx_sample[i] <- mean(mysample$lnSWIR2)
  
  units <- sample.int(nrow(gridAmazonia), size=n, replace=FALSE)
  mz_SI[i] <- mean(gridAmazonia$AGB[units])
}
save(mz, mz_SI, v_mz, mx_sample, file="results/Balanced_Amazonia.RData")
```

```{r SamplingDistributionBalanced, echo=FALSE, fig.asp=.8, fig.cap="Sampling distribution of $\\pi$ estimator of the mean aboveground biomass in Eastern Amazonia, with balanced sampling (balanced on lnSWIR2) and simple random sampling, bot designs with a sample size of 100 units."}
load(file="results/Balanced_Amazonia.RData")
estimates <- data.frame(mz, mz_SI, v_mz, mx_sample)
names(estimates)[c(1,2)] <- c("Balanced","SI")

df <- estimates %>% pivot_longer(.,cols=c("Balanced","SI"))
ggplot(data=df) +
    geom_boxplot(aes(y=value, x=name)) +
    geom_hline(yintercept=mean(gridAmazonia$AGB), colour="red")+
    scale_x_discrete(name="Sampling design") +
    scale_y_continuous(name="Estimated mean AGB")
```

The variance of the 1000 estimates of the population mean of the study variable AGB equals `r as.character(round(var(estimates$Bal),1))`. The gain in precision compared to simple random sampling, equals `r as.character(round(var(estimates$SI)/var(estimates$Bal),1))`, so with simple random sampling three times more sampling units are needed to estimate the population mean with the same precision. The mean of the 1,000 estimated variances equals `r as.character(round(mean(estimates$v_mz),1))`, indicating that the approximate variance estimator somewhat underestimates the true variance in this case. The population mean of the balancing variable lnSWIR2 equals `r round(mean(gridAmazonia$lnSWIR2),3)`. The sample mean of lnSWIR2 varies a bit among the samples (Figure \@ref(fig:histSampleMeanSWIR)). In other words, many samples are not perfectly balanced on lnSWIR2. This is not exceptional, in most cases perfect balance is impossible.

```{r histSampleMeanSWIR, echo=FALSE, fig.cap="Sampling distribution of sample mean of balancing variable lnSWIR2."}
ggplot(data=estimates) +
  geom_histogram(aes(x=mx_sample), binwidth=0.0025, fill="black", alpha=0.5, color="black") +
  scale_y_continuous(name="Frequency") +
  scale_x_continuous(name="Sample mean lnSWIR2")
```
### Stratified random sampling {#StratifiedsamplingasBalancedsampling}

In the previous section a continuous variable was used to balance the sample. However, also a categorical variable can be used for this. A sample balanced on a categorical variable actually is a  stratified random sample. Figure \@ref(fig:BalancedSampleCategorical) shows four strata. These four strata can be used in balanced sampling by constructing the following design matrix\index{Design matrix} $\mathbf{X}$ with as many columns as there are classes:

\begin{eqnarray}
\left[
\begin{array}{cccc}
\pi_{1,1} &0 &0 &0 \\
\pi_{2,1} &0 &0 &0 \\
\pi_{3,1} &0 &0 &0 \\
\pi_{4,1} &0 &0 &0 \\
\ &\pi_{5,2} &0 &0 \\
0 &\pi_{6,2} &0 &0 \\
0 &0 &\pi_{7,3} &0 \\
\vdots &\vdots &\vdots &\vdots\\
0 & 0 & 0 & \pi_{400,4} \\
\end{array}\right]  \;,
\end{eqnarray}

The first four rows refer to the four leftmost bottom row population units in Figure \@ref(fig:BalancedSampleCategorical). These units belong to class A, which explains that the first column for these units contain non-zeroes. These non-zeroes are the inclusion probabilities of the units in stratum A. The other three columns for these rows contain all zeroes. The fifth and sixth unit belong to stratum B, so that the second column for these rows contain the inclusion probabilities for stratum B, and so on. The final row is the upper-right sampling unit in stratum D, so the first three columns contain zeroes, and the fourth column is filled with the inclusion probability of this stratum. The sum of the inclusion probabilities in the first column is the sample size of stratum A. Or reversely, if, for instance, we want to select $n_h$ units from stratum $h$ with equal probability for all units in this stratum, then the inclusion probabilities should equal $n_h/N_h$, with $N_h$ the total number of units in this stratum.

```{r BalancedSampleCategorical, echo=FALSE, fig.cap="Sample balanced on a categorical variable with four classes."}
s1 <- s2 <- 1:20 - 0.5
mypop <- expand.grid(s1, s2)
names(mypop) <- c("s1", "s2")
mypop$stratum <- as.factor(findInterval(mypop$s1, c(4,6,14)))
levels(mypop$stratum) <- LETTERS[1:4]

N_h <- tapply(mypop$s1, INDEX=mypop$stratum, FUN=length)
n <- 20
n_h <- n*N_h/sum(N_h)
labels <- sort(unique(mypop$stratum))
lut <- data.frame(stratum=labels, pi=n_h/N_h)
mypop <- merge(x=mypop, y=lut)

X <- model.matrix(~ stratum-1, mypop)

set.seed(314)
sample_ind <- samplecube(X=X, pik=mypop$pi, comment=FALSE, method=1)
mysample <- mypop[sample_ind==1,]

ggplot(data=mypop) +
  geom_tile(mapping=aes(x=s1, y=s2, fill=factor(stratum)), width=1, height=1, size=0.5, colour="white") +
  geom_tile(data=mysample, mapping=aes(x=s1, y=s2), fill=NA, width=1, height=1, size=0.7, colour="black") +
  scale_fill_viridis_d(name="Stratum") +
  scale_x_continuous(name="Easting") +
  scale_y_continuous(name="Northing") +
  coord_fixed()
```


As a first step inclusion probabilities are computed by $\pi_{hk}=n_h/N_h, k=1, \dots , N_h$, with $n_h=N_h/N$ (proportional allocation). 

```{r, eval=FALSE}
N_h <- tapply(mypop$s1,INDEX=mypop$stratum,FUN=length)
n <- 20
n_h <- n*N_h/sum(N_h)
labels <- sort(unique(mypop$stratum))
lut <- data.frame(stratum=labels, pi=n_h/N_h)
mypop<- merge(x=mypop, y=lut, by="stratum")
```

The design matrix $\mathbf{X}$ is computed with function `model.matrix`, expanding the factor `stratum` to a set of dummy variables. By adding `-1` to the formula, we avoid that the first column in the design matrix has all ones. The design matrix has four columns with dummy variables (indicators), indicating to which stratum a unit belongs. 

The columns in the design matrix with dummy variables are multiplied by the vector with inclusion probabilities, using function `sweep `. This is not strictly needed. Using the design matrix with dummy variables implies that the population totals equal the number of population units in the strata, $N_h$. For a perfectly balanced sample, the sample sums of the balancing variables, the dummy variables,  divided by the inclusion probability are also equal to $N_h$.  Multiplication of the dummy variables with the vector with inclusion probabilities implies that the population totals equal the targeted sample sizes per stratum. For a perfectly balanced sample, the sample sums of the balancing variable (having value $\pi_{hk}$ or 0), divided by the inclusion probability are also equal to $n_h$.

```{r }
X <- model.matrix(~ stratum-1, data=mypop)
X <- sweep(X, MARGIN= 1, mypop$pi, `*`)
set.seed(314)
sample_ind <- samplecube(X=X, pik=mypop$pi, comment=FALSE, method=1)
mysample <- mypop[sample_ind>(1-eps),]
```

In this case all units in a stratum have the same inclusion probability, yielding  a stratified simple random sample. We may also use variable inclusion probabilities, for instance proportional to a size measure of the units, yielding a stratified pps random sample.

The advantage of selecting a stratified random sample by balancing the sample on a categorical variable becomes clear in case we have multiple classifications that we would like to use in stratification, and we cannot afford to use all cross-classifications as strata. This is the topic of the next section.

### Multi-way stratification {#Multiwaystratification}

@Falorsi2008 describe how a multi-way stratified sample\index{Multi-way stratification} can be selected as a balanced sample. Multi-way stratification is of interest when one has multiple stratification variables, each stratification variable leading to several strata, so that the total number of cross-classification strata\index{Cross-classification strata} becomes so large that the stratum sample sizes are strongly disproportional to their size, or even exceed the total sample size.

Let $M$ be the sum of the number of map units over all maps used for stratification.  For instance, if we have three maps with $4+3+6$ map units, $M$ equals 13.  Instead of using all cross-classification map units as strata, the $M$ map units are used as strata.  The sample sizes of the marginal strata can be controlled by using a design matrix with as many columns as there are strata. The units of an individual map used for stratification are referred to as marginal strata\index{Marginal strata}. Each row $k = 1, \dots, N$ in the design matrix  $\mathbf{X}$ has as many non-zero values as we have maps, in entries corresponding with the cross-classification map unit population unit $k$ belongs to, and zeroes in the remaining entries. The non-zero value is the inclusion probability of that unit. The inclusion probability of a unit is independent of the map used for stratification, so the non-zero values in a row are all equal. Each column of the design matrix has non-zero values at entries corresponding with the population units in that marginal stratum, and zeroes at all other entries.

Two-way stratified random sampling is illustrated with a simulated population of 400 units (Figure \@ref(fig:TwowaystratifiedPopulation)).

```{r TwowaystratifiedPopulation, echo=FALSE, fig.cap="Simulated population, used for illustration of two-way stratified random sampling."}
s1 <- s2 <- 1:20 - 0.5
mypop <- expand.grid(s1, s2)
names(mypop) <- c("s1", "s2")

library(gstat)
vgmodel <- vgm(model="Exp", psill=4, range=4, nugget=0)

dists1 <- outer(mypop$s1, mypop$s1, FUN="-")
dists2 <- outer(mypop$s2, mypop$s2, FUN="-")
dist <- sqrt(dists1^2+dists2^2)
C <- variogramLine(vgmodel, dist_vector=dist, covariance=TRUE)

Upper <- chol(C)
set.seed(314)
N <- nrow(mypop)
G <- rnorm(N,0,1) #simulate random numbers from standard normal distribution
mypop$z <- crossprod(Upper,G)+10

mypop$A <- as.factor(findInterval(mypop$s1, c(5,8,12)))
mypop$B <- as.factor(findInterval(mypop$s2, c(8,15)))
levels(mypop$A) <- c("A1","A2","A3","A4")
levels(mypop$B) <- c("B1","B2","B3")

ggplot(data=mypop) +
  geom_tile(mapping=aes(x=s1, y=s2, fill=z), width=1, height=1, size=0.5) +
  scale_fill_continuous(type="viridis") +
  scale_x_continuous(name="Easting") +
  scale_y_continuous(name="Northing") +
  coord_fixed()
```

Figure \@ref(fig:Twowaystratifiedsample) shows two classifications of the population units. Classification A consists of four classes (map units), classification B of three classes. Instead of using $4 \times 3 = 12$ cross-classifications as strata in random sampling, only $4+3=7$ marginal strata are used in two-way stratified random sampling.

As a first step the inclusion probabilities are added to the data frame `mypop` with the spatial coordinates and simulated values. To keep it simple I computed inclusion probabilities equal to two divided by the number of population units in a cross-classification stratum. Note that this does not imply that a sample is selected with two units per cross-stratum. As we will see later it is possible that in some  cross-classification stratum no units are selected at all, while in other cross-classification strata more than two units are selected. In multi-way stratified sampling the marginal stratum sample sizes are controlled. The inclusion probabilities  should result in six selected units for all four units of map A, and eight selected units for all three units of map B.   

```{r}
mypop <- mypop %>%
    group_by(A, B) %>%
    summarise(N_h=n(), .groups="drop") %>%
    mutate(pih=rep(2,12)/N_h) %>%
    right_join(mypop, by=c("A","B"))
```

The next step is to create the design matrix. Two submatrices are computed, one per stratification. The two submatrices are joined column-wise, using function `cbind`.  The columns are multiplied by the vector with inclusion probabilities.

```{r}
XA <- model.matrix(~A-1, mypop)
XB <- model.matrix(~B-1, mypop)
X <- cbind(XA,XB)
X <- sweep(X, MARGIN= 1, mypop$pih, `*`)
```

Matrix $\mathbf{X}$ can be reduced by one column if in the first column the inclusion probabilities of *all* population units are inserted. This first column contains no zeroes. Balancing on this variable implies that the total sample size is controlled. Now there is no need anymore to control the sample sizes of all marginal strata. It is sufficient to control the sample sizes of three marginal strata of map A and two marginal strata of map B.

```{r}
X <- model.matrix(~A+B, mypop)
X <- sweep(X, MARGIN= 1, mypop$pih, `*`)
```

This reduced design matrix is not strictly needed for selecting a multi-way stratified sample, but must be used in estimation. If in estimation as many balancing variables are used as we have marginal strata, the matrix with the sum of squares of balancing variables in Equation \@ref(eq:betasbalanced)) cannot be inverted (matrix is singular), and as a consequence the population regression coefficients cannot be estimated.

Finally, the two-way stratified random sample is selected with function `samplecube` of package **sampling** [@Tille2016]. 

```{r}
sample_ind <- samplecube(X=X, pik=mypop$pih, method=1, comment=FALSE)
eps <- 1e-6
units <- which(sample_ind>(1-eps))
mysample <- mypop[units,]
```

Figure \@ref(fig:Twowaystratifiedsample) shows the selected sample.

```{r, Twowaystratifiedsample, echo=FALSE, out.width='100%', fig.cap="Two-way stratified sample."}
plt1 <- ggplot(data=mypop) +
  geom_tile(mapping=aes(x=s1, y=s2, fill=factor(A)), width=1, height=1, size=0.5, colour="white") +
  geom_tile(data=mysample, mapping=aes(x=s1, y=s2), fill=NA, width=1, height=1, size=0.7, colour="black") +
  scale_fill_viridis_d(name="Stratum") +
  scale_x_continuous(name="") +
  scale_y_continuous(name="") +
  coord_fixed()

plt2 <- ggplot(data=mypop) +
  geom_tile(mapping=aes(x=s1, y=s2, fill=factor(B)), width=1, height=1, size=0.5, colour="white") +
  geom_tile(data=mysample, mapping=aes(x=s1, y=s2), fill=NA, width=1, height=1, size=0.7, colour="black") +
  scale_fill_viridis_d(name="Stratum") +
  scale_y_continuous(name="") +
  scale_x_continuous(name="") +
  coord_fixed()

grid.arrange(plt1, plt2, nrow=1)
```

All marginal sample sizes\index{Marginal sample size} of map A are six, and all marginal sample sizes of map B are eight, as expected. The sample sizes of the cross-classification strata vary from zero to four.

```{r}
addmargins(table(mysample$A, mysample$B))
```

The population mean can be estimated by the $\pi$ estimator.

```{r}
N <- nrow(mypop)
print(mean <- sum(mysample$z/mysample$pih)/N)
```

The variance is estimated as before (Equation \@ref(eq:approxvarianceBalanced)).

```{r}
c <- (1-mysample$pih)
b <- estimate_b(z=mysample$z/mysample$pih, X=X[units,]/mysample$pih, c=c)
zpred <- X%*%b
e <- mysample$z-zpred[units]
n <- nrow(mysample)
v_tz <- n/(n-ncol(X))*sum(c*(e/mysample$pih)^2)
print(v_mz <- v_tz/N^2)
```


#### Exercises {-}

1. Spatial clustering of sampling units is not avoided in balanced sampling. What effect do you expect of this spatial clustering on the precision of the estimated mean? Can you think of a situation where this effect does not occur?   

## Well-spread sampling {#Spreaded}

With balanced sampling the spreading of the sampling units in the space spanned by the balancing variables can be poor. For instance, in Figure \@ref(fig:simpleexample)(a) the Easting coordinates of all units of a sample balanced on Easting can be equal or close to the population mean of 10. So, in this example balancing does not guarantee a good geographical spreading. A balanced sample can be selected that shows strong clustering in the space spanned by the balancing variables. This clustering may inflate the standard error of the estimated population total and mean. The clustering in geographical or covariate space can be avoided by the local pivotal method [@Grafstrom2012], and the spatially correlated Poisson sampling method\index{Spatially correlated Poisson sampling} [@Grafstrom2012b]. 

For spreading in *geographical* space various other designs are available. A simple design is stratified random sampling from compact geographical strata, see Section \@ref(geostrata). Alternative designs are generalised random-tessellation stratified sampling [@stevens2004], and balanced acceptance sampling\index{Balanced acceptance sampling} [@Robertson2013].

### Local pivotal method {#LPM}

The local pivotal method (LPM) is a modification of the pivotal method explained in Section \@ref(ppswor)\index{Local pivotal method for well-spread sampling}. The only difference with the pivotal method is the selection of the pairs of units. In the pivotal method at each step two units are selected, for instance, the first two units in the vector with inclusion probabilities after randomizing the order of the units. In the local pivotal method the first unit is selected fully randomly and the nearest neighbour of this unit is used as its counterpart. Recall that when one unit of a pair is included in the sample, the inclusion probability of its counterpart is decreased. This leads to a better spreading of the sampling units in the space spanned by the spreading variables.

LPM can be used for arbitrary inclusion probabilities. The inclusion probabilities can be equal, but as in the pivotal method these probabilities may also differ among the population units.   

Selecting samples with LPM can be done with functions `lpm`, `lpm1` or `lpm2` of package **BalancedSampling** [@Grafstrom2016]. The functions `lpm1` and `lpm2` only differ in the selection of neighbours that are allowed to compete, for details see @Grafstrom2012. For most populations the two algorithms perform similar (personal communication Anton Grafstr&ouml;m). The algorithm implemented in the function `lpm` is only recommended when the population size is too large for `lpm1` or `lpm2`. It only uses a subset of the population in search for nearest neighbours, and is thus not as good. Another function ` lpm2_kdtree` of package  **SamplingBigData** [@samplingBigData] is developed for big data sets.

Inclusion probabilities are computed with function `inclusionprobabilities` of package **sampling**.  A matrix $\mathbf{X}$ must be defined with the values of the spreading variables of the population units. Figure \@ref(fig:LPMKandahar) shows a sample of 40 units selected from the sampling frame of Kandahar, using the spatial coordinates of the population units as spreading variables. Inclusion probabilities are proportional to the agricultural area within the population units. The geographical spreading is improved compared with the sample shown in Figure \@ref(fig:ppswrKandahar). 

```{r}
library(BalancedSampling)
library(sampling)
load("data/Kandahar.RData")
n <- 40
pi <- inclusionprobabilities(grdKandahar$agri, n)
X <- cbind(grdKandahar$x, grdKandahar$y)
set.seed(314)
units <- lpm1(pi, X)
myLPMsample <- grdKandahar[units,]
```

```{r LPMKandahar, echo=FALSE, out.width='100%', fig.cap="Spatial ppswor sample selected by local pivotal method, using agricultural area as size variable."}
ggplot(data=grdKandahar) +
  geom_raster(mapping=aes(x=x, y=y, fill=agri)) +
  geom_tile(data=myLPMsample, mapping=aes(x=x, y=y), colour="white", size=0.7, width=5, height=5, fill=NA) +
  scale_fill_viridis_c(name="Agric. area") +
  scale_x_continuous(name="Easting (km)") +
  scale_y_continuous(name="Northing (km)") +
  coord_fixed()
```

The total poppy area can be estimated with the $\pi$ estimator (Equation \@ref(eq:HTTotalppswor)).

```{r}
myLPMsample$pi <- pi[units]
tz_HT <- sum(myLPMsample$poppy/myLPMsample$pi)
```

The estimated total poppy area equals `r as.character(round(tz_HT,0))` ha. The sampling variance of the estimator of the population total with the local pivotal method can be estimated by [@Grafstrom2014]

\begin{equation}
\widehat{V}(\hat{t}(z)) = \frac{1}{2} \sum_{k \in \mathcal{S}} \left( \frac{z_k}{\pi_k} - \frac{z_{k_j}}{\pi_{k_j}} \right)^2 \;,
(\#eq:VartotalLPM)
\end{equation}

with ${k_j}$ the nearest neighbour of unit $k$ in the sample. This variance estimator is for the case where we have only one nearest neighbour.

Function `vsb` of package **BalancedSampling** is an implementation of a more general variance estimator that accounts for more than one nearest neighbour (equation 6 in @Grafstrom2014). We expect a somewhat smaller variance compared to pps sampling, so we may use the variance of the pwr estimator (Equation \@ref(eq:VarHHTotalppswr)) as a conservative variance estimator\index{Conservative variance estimator}.

```{r vartotLPM}
Xsample <- X[units,]
se_tz_HT <- sqrt(vsb(pi[units], myLPMsample$poppy, Xsample))
pk <- myLPMsample$pi/n
se_tz_pwr <- sqrt(var(myLPMsample$poppy/pk)/n)
```

The standard error obtained with function `vsb` equals `r as.character(round(se_tz_HT,0))`, the standard error of the Hansen-Hurwitz estimator equals  `r as.character(round(se_tz_pwr,0))`. So in this case the Hansen-Hurwitz variance estimator is smaller than the other variance estimator, but on average it will be larger.

As explained above, the LPM design can also be used to select a probability sample well-spread in the space spanned by one or more quantitative covariates. Matrix $\mathbf{X}$ then should contain the values of the *scaled* (standardised) covariates instead of the spatial coordinates.

#### Exercises {-}

2. Geographical spreading of the sampling units can also be achieved by random sampling from compact geographical strata (Section \@ref(geostrata)). Can you think of one or more advantages of LPM sampling over random sampling from geostrata?  

### Generalised random-tessellation stratified sampling {#GRTS}

Generalised random-tessellation stratified sampling\index{Generalised random-tessellation stratified sampling} (GRTS) is designed for sampling discrete objects scattered throughout space, think for instance of the lakes in Finland, segments of hedgerows in England etc.  Each object is represented by a point in 2D-space. It is a complicated design, and for sampling points from a continuous universe, or raster cells from a finite population, I recommend more simple designs such as the local pivotal method (Section \@ref(LPM)), balanced sampling with geographical spreading (Section \@ref(BalancedandSpreaded)), or sampling from compact geographical strata (Section \@ref(geostrata)).  Let me try to explain the GRTS design with a simple example of a finite population of point objects in a circular study area (Figure  \@ref(fig:GRTSNumbering)). For a more detailed description of this design I refer to @Hankin2019. As a first step a square bounding box of the study area is constructed. This bounding box is recursively partitioned into square grid cells. First 2 x 2 grid cells are constructed. These grid cells are numbered in a predefined order. In Figure  \@ref(fig:GRTSNumbering)(b) this numbering is from lower left, lower right, upper left to upper right. Each grid cell is then subdivided into four subcells; the subcells are numbered using the same order. This is repeated until at most one population unit occurs in each subcell. For our population only two iterations were needed, leading to 4 x 4 subcells. Note that in some subcells no population unit occurs. Each address of a subcell consists of two digits, the first digit is for the grid cell, the second digit for the subcell. 

```{r GRTSNumbering, echo=FALSE, out.width="100%", fig.cap = "Numbering of grid cells and subcells for GRTS sampling."}
s1 <- s2 <- 1:100 - 0.5
pxl <- expand.grid(s1, s2)
names(pxl) <- c("s1", "s2")

#construct strata level 1
s1bnd <- 50
s1f <- findInterval(pxl$s1, s1bnd)
s2f <- findInterval(pxl$s2, s1bnd)
pxl$partit1 <- factor(interaction(s1f, s2f), labels=c(1,2,3,4))

#construct strata level 2
s1bnd <- seq(from=25, to=75, by=25)
s1f <- findInterval(pxl$s1, s1bnd)
s2f <- findInterval(pxl$s2, s1bnd)
lbl <- c("00","01","10","11","02","03","12","13","20","21","30","31","22","23","32","33")
pxl$partit2 <- factor(interaction(s1f,s2f), labels=lbl)

n_h <- rep(1,times=16)
set.seed(314)
units <- sampling::strata(pxl,stratanames="partit2",
                size=n_h,method="srswor")
myfinpop <- getdata(pxl,units)
#remove some units, and shift fourth point into circle
myfinpop$s1[4] <- 78
units <- sample.int(nrow(myfinpop),2)
myfinpop <- myfinpop[-units,]
N <- nrow(myfinpop)

circle <- data.frame(s1=50, s2=50)
plt1 <- ggplot(data=myfinpop) +
  geom_raster(data=pxl, mapping=aes(x=s1,y=s2), fill="lightgrey")+
  geom_circle(data=circle, aes(x0=s1, y0=s2, r=50), fill="grey")+
  geom_point(mapping=aes(x=s1, y=s2), size=2) +
  scale_fill_discrete(guide="none") +
  coord_fixed()

dat <- expand.grid(x=seq(from=12.5, to=87.5, by=25), y=seq(from=12.5,to=87.5,by=25))
dat$labels <- lbl

plt2 <- ggplot(data=myfinpop) +
  geom_raster(data=pxl, mapping=aes(x=s1, y=s2, fill=as.factor(partit1)), alpha=0.5)+
  geom_circle(data=circle, mapping=aes(x0=s1, y0=s2, r=50))+
  geom_vline(xintercept=c(25,50,75))+
  geom_hline(yintercept=c(25,50,75))+
  geom_point(mapping=aes(x=s1, y=s2), size=2) +
  geom_text(data=dat,mapping=aes(x=x, y=y, label=labels)) +
  scale_fill_viridis_d(guide="none") +
  coord_fixed()

grid.arrange(plt1, plt2, nrow=1)
```

The next step is to place the sixteen subcells on a line in a random order. The randomisation is done hierarchically. First the four grid cells at the highest level are randomized. In our example the randomized order is 1, 2, 3, 0 (Figure \@ref(fig:GRTS)). Then within each grid cell the order of the subcells is randomized. This is done independently for the grid cells. In our example for grid cell 1 the randomized order of the subcells is 2, 1, 3, 0 (Figure \@ref(fig:GRTS)). Note that the empty subcells, subcells (0,0) and (3,3) are removed from the line.

```{r}
set.seed(314)
ord <- sample.int(4,4)
myfinpop_rand <- NULL
for (i in ord) {
  units <- which(myfinpop$partit1==i)
  units_rand <- sample(units, size=length(units))
  myfinpop_rand <- rbind(myfinpop_rand, myfinpop[units_rand,])
}
```

After the subcells are placed on a line, a one-dimensional systematic random sample is selected (Figure \@ref(fig:GRTS)), see also Section \@ref(Systematicpps). This can be done either with equal or unequal inclusion probabilities. With equal inclusion probabilities the length of the lines representing the population units is constant. With unequal inclusion probabilities the length of the lines is proportional to a size variable. For a sample size of $n$, the total line is divided into $n$ segments of equal length. A random point is selected in the first segment, and the other points of the systematic sample are determined. Finally, the population units corresponding with the selected systematic sample are identified. With equal probabilities the five selected units are the units in subcells 11, 23, 22, 32 and 03 (Figure \@ref(fig:GRTS)). 

```{r sys1D}
size <- rep(1,N)
n <- 5
interval <- sum(size)/n
start <- round(runif(1)*interval,2)
mysys <- c(start,1:(n-1)*interval+start)
```

```{r GRTS, echo=FALSE, fig.asp=.15, fig.cap = "Systematic random sample along a line with equal inclusion probabilities."}
cellbound <- c(0, cumsum(size))
#compute width of cells
w <- diff(cellbound)
#compute x coordinates halfway the cell boundaries
x <- cellbound[1:N]+w/2

n1 <- table(myfinpop_rand$partit1)
address_1 <- rep(ord, n1[ord])
address <- myfinpop_rand$partit2

datfr <- data.frame(x, y=rep(1,N), address_1, address, w)
ggplot(data=datfr) +
  geom_tile(mapping=aes(x=x, y=y, fill=factor(address_1), width=w), alpha=0.5) +
  geom_text(mapping=aes(x=x, y=y, label=address)) +
  geom_vline(xintercept=cellbound, linetype=2)+
  scale_y_continuous("", breaks=c())+
  scale_x_continuous("", breaks=mysys)+
  scale_fill_viridis_d(guide="none")
```
Figure \@ref(fig:GRTSpps) shows a systematic random sample along a line with unequal inclusion probabilities. The inclusion probabilities are proportional to a size variable, with values 1, 2, 3 or 4. The selected population units are the units in subcells 10, 20, 31, 01 and 02.

```{r GRTSpps, echo=FALSE, fig.asp=.15, fig.cap = "Systematic random sample along a line with inclusion probabilities proportional to size."}
size <- sample.int(4, size=N, replace=TRUE)
n <- 5
interval <- sum(size)/n
start <- round(runif(1)*interval,2)
mysyspps <- c(start,1:(n-1)*interval+start)

cellbound <- c(0,cumsum(size))
w <- diff(cellbound)
x <- cellbound[1:N]+w/2
datfr <- data.frame(x, y=rep(1,N), address_1, address, w)
ggplot(data=datfr) +
  geom_tile(mapping=aes(x=x, y=y, fill=factor(address_1), width=w), alpha=0.5) +
  geom_text(mapping=aes(x=x, y=y, label=address)) +
  geom_vline(xintercept=cellbound, linetype=2)+
  scale_y_continuous("", breaks=c())+
  scale_x_continuous("", breaks=mysyspps)+
  scale_fill_viridis_d(guide="none")
```

GRTS samples can be selected with function `grts` of package **spsurvey** [@spsurvey]. The next code chunk shows the selection of a GRTS sample of 40 units from Kandahar. First a data frame is created representing the sampling frame. With unequal inclusion probabilities this data frame  must include a variable for the inclusion probabilities. Next, a named list specifying the sampling design is created. The first element specifies how many units must be selected. In case of stratified random sampling these sample sizes must be set per stratum. Also, more than one sample can be selected (per stratum), referred to as panels. Per sampling round only one panel is observed. After multiple rounds the sample data can be used for estimating the temporal change of the spatial mean or total. The element `seltype` in the design list must be set to "Continuous" for sampling with probabilities proportional to an ancillary variable specified with argument `mdcaty`. If the argument `shift.grd` is set to TRUE (the default value), the hierarchical grid is shifted to a random position.

```{r GRTSppsKandahar}
library(spsurvey)
n <- 40
pi <- inclusionprobabilities(grdKandahar$agri, n)
N <- nrow(grdKandahar)
samplingframe <- data.frame(
  x=grdKandahar$x, y=grdKandahar$y, mdcaty=pi, ids=1:N)
design <- list(None=list(panel=c(PanelOne=n), seltype="Continuous"))
set.seed(314)
res <- grts(
  design, type.frame="finite", src.frame="att.frame",
  att.frame=samplingframe, xcoord="x", ycoord="y",
  mdcaty="mdcaty", do.sample=TRUE, shapefile=FALSE)
units <- res$ids
myGRTSsample <- grdKandahar[units,]
```

The total poppy area is estimated by the $\pi$ estimator.

```{r estimationGRTSKandahar}
tz_GRTS <- sum(myGRTSsample$poppy/pi[units])
```

The estimated total is `r as.character(round(tz_GRTS,0))`. Function `vsb` of package **BalancedSampling** can be used to estimate the standard error of the estimated total poppy area.

```{r}
X <- cbind(grdKandahar$x, grdKandahar$y)
Xsample <- X[units,]
sqrt(vsb(pi[units], myGRTSsample$poppy, Xsample)) %>% round(.,0)
```


## Balanced sampling with spreading {#BalancedandSpreaded}

As mentioned in the introduction to this chapter a sample balanced on a covariate still may have a poor spreading along the axis spanned by the covariate. @Grafstrom2013 presented a method for selecting balanced samples that are also well-spread in the space spanned by the covariates, which they refer to as doubly-balanced sampling\index{Doubly-balanced sampling}. If we take one or more covariates as balancing variables, and besides Easting and Northing as spreading variables, this leads to balanced samples with good *geographical* spreading. When the residuals of the regression model show spatial structure (are spatially correlated), the estimated population mean of the study variable becomes more precise thanks to the improved geographical spreading. Balanced samples with spreading can be selected with function `lcube` of package **BalancedSampling** [@Grafstrom2016]. This is illustrated with Eastern Amazonia, using as before lnSWIR2 for balancing the sample.  

```{r lcube}
library(BalancedSampling)
N <- nrow(gridAmazonia)
n <- 100
Xbal <- cbind(rep(1,times=N), gridAmazonia$lnSWIR2)
Xspread <- cbind(gridAmazonia$x1, gridAmazonia$x2)
pi <- rep(n/N, times=N)
set.seed(314)
units <- lcube(Xbal=Xbal, Xspread=Xspread, prob=pi)
mysample <- gridAmazonia[units,]
```

```{r DoublyBalanceSampleAmazonia, echo=FALSE, out.width='100%', fig.cap="Balanced sample, balanced on lnSWIR2, with geograhical spreading from Eastern Amazonia (Brazil)."}
ggplot(data=gridAmazonia) +
  geom_raster(mapping=aes(x=x1/1000, y=x2/1000, fill=lnSWIR2)) +
  geom_point(data=mysample, mapping=aes(x=x1/1000, y=x2/1000), size=2)+
  scale_fill_continuous(name="lnSWIR2", type="viridis") +
  scale_x_continuous(name="Easting (km)") +
  scale_y_continuous(name="Northing (km)") +
  coord_fixed()
```
Comparing this sample with the balanced sample in Figure \@ref(fig:BalancedSampleAmazonia) shows that the geographical spreading of the sample is improved, although there still are some close points. The $\pi$ estimate of the mean is `r mean(mysample$AGB)`.

The variance of the estimator of the mean can be estimated by (equation 7, @Grafstrom2013)

\begin{equation}
\widehat{V}(\hat{\bar{z}}) = \frac{n}{n-p}\frac{p}{p+1} \sum_{k \in \mathcal{S}}(1-\pi_k) \left(\frac{\epsilon_k}{\pi_k}-\bar{\epsilon}_k \right)^2 \;,
(\#eq:VarmeanDoublyBalanced)
\end{equation}

with $p$ the number of balancing variables, $\epsilon_k$ the regression model residual of unit $k$ (Equation \@ref(eq:residualsBalanced)), and $\bar{\epsilon}_k$ the local mean of the residuals of this unit, computed by

\begin{equation}
\bar{\epsilon}_k = \frac{\sum_{j=1}^{p+1}(1-\pi_j)\frac{\epsilon_j}{\pi_j}}{\sum_{j=1}^{p+1}(1-\pi_j)}\;.
(\#eq:localmeanresdual)
\end{equation}

This variance estimator is easy to compute with functions `localmean.weight` and `localmean.var` of package **spsurvey** [@spsurvey].

```{r}
library(spsurvey)
pi <- rep(n/N,n)
c <- (1-pi)
b <- estimate_b(z=mysample$AGB/pi, X=Xbal[units,]/pi, c=c)
zpred <- Xbal%*%b
e <- mysample$AGB-zpred[units]
weights <- localmean.weight(x=mysample$x1, y=mysample$x2, prb=rep(pi,n), nbh=3)
v_mz <- localmean.var(z=e/pi, weight.lst=weights)/N^2
```

The estimated standard error is `r as.character(round(sqrt(v_mz),1))`, which is considerably smaller than the estimated standard error of the balanced sample without geographical spreading.

```{r, echo=FALSE}
rm(list=ls())
```
