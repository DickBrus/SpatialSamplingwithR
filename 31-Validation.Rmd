# Sampling for validation of maps {#Validation}

In the previous chapters of Part II various methods are described for selecting sampling units with the aim to map the study variable. Once the map has been made, we would like to know how good it is. It should come as no surprise that the value of the study variable at a randomly selected location as shown on the map differs from the value at that location in reality. This difference is a prediction error. The question is how large this error is on average, and how variable it is. In this chapter I describe and illustrate with a real-world case study how to select sampling units at which we will confront the predictions with the true values, and how to estimate map quality indices from the prediction errors at these sampling locations.

If the map has been made with a statistical model, then the predictors are typically model-unbiased and the variance of the prediction errors can be computed from the model. Think for instance of kriging, which also results in a map of the kriging variance. In Chapters \@ref(MBgridspacing) and \@ref(MBSamplePattern) I showed how this kriging variance can be used to optimise the grid spacing (sample size) and the spatial coordinates of the sampling units for mapping. So, if we have a map of these variances, why do we still need to collect new data for estimating the map quality?

The problem is that these kriging variances rely on the validity\index{Validity} of the assumptions made in modelling the spatial variation of the study variable. Do we assume a constant mean, or a mean that is linear combination of some covariates? Which covariates? Or should we model the mean with a non-linear function, as in random forests? How good is our estimate of the semivariogram model form and parameters, either for the target variable or the residuals from some function of the covariates? If one or more of these modelling assumptions are violated, the variances of the prediction errors may become biased. For this reason the quality of the map is preferably determined through independent validation, i.e. by comparing predictions with observations not used in mapping, and followed by *model-free* design-based estimation of the map quality indices. This process is often referred to as validation\index{Validation}, perhaps better statistical validation, a subset of the more comprehensive term map quality evaluation\index{Map quality evaluation}, which includes the concept of fitness-for-use. 

Statistical validation of maps is often done through data-splitting\index{Data-splitting} or cross-validation\index{Cross-validation}. In data-splitting the data are split into two subsets, one for calibrating the model and mapping, one for validation. In cross-validation the data set is split into a number of disjoint subsets of equal size. Each subset is used one-by-one for calibration and prediction. The remaining subsets are used for validation. Leave-one-out-crossvalidation\index{Cross-validation!leave-one-out-crossvalidation} (LOOCV) is a special case of this, in which each sampling unit is left out one-by-one, and all other units are used for calibration and prediction of the study variable of the unit that is left out. The problem with data-splitting and cross-validation is that the data used for mapping typically are from non-probability samples. This makes model-free estimation of the map quality indices unfeasible [@brus2011d]. Designing a sampling scheme starts with a comprehensive description of the aim of the sampling project [@gru06]. Mapping and validation are different aims, which ask for different sampling approaches. For validation probability sampling is the best option because then a statistical model of the spatial variation of the prediction errors is not needed. Map quality indices, defined as population parameters, can be estimated model-free, by design-based inference (see also Section \@ref(DBvsMB)).

## Map quality indices

I would like to emphasise that in validation we want to assess the accuracy of the map as a whole. We are not interested in the accuracy at a sample of population units only. For instance, we would like to know the prediction error averaged over all population units and not merely the average prediction error at a sample of units. Map quality indices are therefore defined as population parameters. Because we cannot afford to determine the prediction error for each unit of the mapping area to calculate these population means (if we could do that there would be no need for a mapping model), we have to take a sample of units at which the predictions of the study variable are confronted with the observations. This sample is then used to *estimate* population parameters of the prediction error and our uncertainty about these population parameters, as quantified, for instance, by their standard errors or confidence interval.

For quantitative maps, i.e. maps depicting a quantitative study variable\index{Quantitative map}, popular map quality indices\index{Map quality indices} are (i) the population mean error\index{Population mean error} (ME); (ii) the population mean absolute error\index{Population mean absolute error} (MAE); and (iii) the population mean squared error\index{Population mean squared error}, defined as

\begin{align}
ME = \frac{1}{N}\sum_{k=1}^N (\hat{z}_k-z_k) \\
MAE = \frac{1}{N}\sum_{k=1}^N (|\hat{z}_k-z_k|) \\
MSE = \frac{1}{N} \sum_{k=1}^N (\hat{z}_k-z_k)^2\;,
(\#eq:mapqualityindices)
\end{align}

with $N$ the total number of units (e.g. pixels) in the population, $\hat{z}_k$ the predicted value for unit $k$, $z_k$ the true value of that unit and $|\cdot|$ the absolute value operator. For infinite populations the sum must be replaced by an integral. The ME quantifies the systematic error\index{Systematic error} and ideally  equals 0. It can be positive (in case of overprediction)  and negative (in case of underprediction). Positive and negative errors cancel out and, as a consequence, the ME does not quantify the magnitude of the prediction errors. The MAE and RMSE do quantify the magnitude of the errors, they are nonnegative. Often the square root of MSE is taken, denoted by RMSE, which is in the same units as the study variable and is therefore more intelligible. The RMSE is strongly affected by outliers (large prediction errors), due to the squaring of the errors, and for this reason it is recommended to estimate both MAE and RMSE.

Two other important map quality indices are the population coefficient of determination\index{Population coefficient of determination} and the Nash-Sutcliffe model efficiency coefficient\index{Model efficiency coefficient} (MEC). The population coefficient  of determination $R^2$ is defined as the square of the Pearson correlation coefficient $r$ of the study variable and the predictions of the study variable, given by

\begin{equation}
r = \frac{\sum_{k=1}^{N}(z_k - \bar{z})(\hat{z}_k-\bar{\hat{z}})}{\sqrt{\sum_{k=1}^{N}(z_k- \bar{z})^2}\sqrt{(\hat{z}_k-\bar{\hat{z}})^2}}=\frac{S^2(z,\hat{z})}{S(z)S(\hat{z})}\;,
(\#eq:r)
\end{equation}

with $\bar{z}$ the population mean of the study variable, $\bar{\hat{z}}$ the population mean of the predictions, $S^2(z,\hat{z})$ the population covariance of the study variable and the predictions of $z$, $S(z)$ the population standard deviation of the study variable and $S(\hat{z})$ the population standard deviation of the predictions. Note that $R^2$ is unaffected by bias and therefore should not be used in isolation, but always accompanied by ME.

MEC is defined as [@Janssen1995]

\begin{equation}
MEC=1-\frac{\sum_{k=1}^{N}(\hat{z}_k - z_k)^{2}}{\sum_{k=1}^{N}(z_k -\bar{z})^{2}}=1-\frac{MSE}{S^2(z)} \;,
(\#eq:MEC)
\end{equation}

with $S^2(z)$ the population variance\index{Population variance} of the study variable. MEC quantifies the improvement made by the model over using the mean of the observations as prediction. A value of one (MEC = 1) indicates a perfect match between the measured and predicted values of the study variable, whereas a value of 0 indicates that the mean of the measured values is as good a predictor as the model. A negative value occurs when the mean of the measured values is a better predictor than the model, i.e.  when the residual variance is larger than the variance of the measurements.

For categorical maps\index{Categorical map} a commonly used map quality index is the overall purity\index{Overall purity}, which is defined as the proportion of units (finite population) or fraction of the area (infinite populations) that is correctly classified (mapped):

\begin{equation}
P = \frac{1}{N}\sum_{k=1}^N y_k\;,
(\#eq:Purity)
\end{equation}

with $y_k$ an indicator for unit $k$, having value 1 if the predicted class equals the true class, and 0 otherwise:

\begin{equation}
y_k = \left\{
\begin{array}{cc}
1 & \;\;\;\mathrm{if}\;\;\; \hat{c}_k = c_k\\
0 & \;\;\;\mathrm{otherwise}\;,
\end{array}
\right.
(\#eq:indfromy)
\end{equation}

with $c_k$ and $\hat{c}_k$ the true and predicted class of unit $k$, respectively.

The population ME, MSE, $R^2$, MEC and purity can also be defined for subpopulations. For categorical maps natural subpopulations are the classes depicted in the map, the map units. In that case the purity of map unit $u$ is defined as the fraction of the area of map unit $u$ that is correctly mapped as $u$.

A different subpopulation is the part of the population that is *in reality* class $u$ (but possibly not mapped as $u$). We are interested in the fraction of the area covered by this subpopulation that is correctly mapped as $u$. This is referred to as the class representation\index{Class representation} of class $u$, for which I use hereafter the symbol $R_u$.

### Estimation of map quality indices

The map quality indices are defined as population or subpopulation means. To estimate these (sub)population means a design-based sampling approach is most appropriate. Sampling units are selected by probability sampling and the map quality indices are estimated by model-free design-based inference. So, the ME of a finite population can be estimated by the $\pi$ estimator (see Equation \@ref(eq:HTMean)):

\begin{equation}
\widehat{ME} =\frac{1}{N} \sum_{k \in \mathcal{S}} \frac{1}{\pi_k}e_k \;,
(\#eq:HTME)
\end{equation}

with $e_k = \hat{z}_k-z_k$ the prediction error for unit $k$. By taking the absolute value of the prediction errors $e_k$ in Equation \@ref(eq:HTME) or by squaring them, the $\pi$ estimators for the MAE and MSE are obtained, respectively. By replacing $e_k$ by the indicator $y_k$ of Equation \@ref(eq:indfromy), the $\pi$ estimator for the overall purity is obtained.

With simple random sampling the square of the sample correlation coefficient, i.e. the correlation of the study variable and the predictions of the study variable in the sample, is an unbiased estimator of $R^2$. See @sar92 (p. $486-491$) for how to estimate $R^2$ for other sampling designs.

The population MEC can be estimated by

\begin{equation}
\widehat{MEC}=1-\frac{\widehat{MSE}}{\widehat{S^2}(z)}\;,
(\#eq:HTMEC)
\end{equation}

For simple random sampling the sample variance\index{Sample variance}, i.e. the variance of the observations of $z$ in the sample, is an unbiased estimator of the population variance $S^2(z)$. For other sampling designs this population variance can be estimated by Equation \@ref(eq:EstimatorPopulationVariance4AnyDesign).

Estimation of the class representations is slightly more difficult because the sizes of the classes (number of pixels or area where in reality class $u$ is present) are unknown, and therefore must also be estimated from the sample. This leads to the estimator of a ratio [@brus2011d]:

\begin{equation}
\hat{R}_{u,\text{ratio}}=\frac{\sum_{k \in \mathcal{S}}\frac{y_k}{\pi_k}}{\sum_{k \in \mathcal{S}}\frac{x_k}{\pi_k}}\;,
\label{RatioEstimatorClassRepresentation}
\end{equation}

where $y_{u,k}$ denotes an indicator defined as

\begin{equation}
y_{u,k} = \left\{
\begin{array}{cc}
1 & \;\;\;\mathrm{if}\;\;\; \hat{c}_k = c_k =  u\\
0 & \;\;\;\mathrm{otherwise}\;,
\end{array}
\right.
(\#eq:indicatorfromy)
\end{equation}

and $x_k$ denotes an indicator defined as

\begin{equation}
x_k = \left\{
\begin{array}{cc}
1 & \;\;\;\mathrm{if}\;\;\; c_k = u\\
0 & \;\;\;\mathrm{otherwise}\;.
\end{array}
\right.
(\#eq:indicatorfromx)
\end{equation}

This estimator is also recommended in situations where the sample size is not fixed but varies among samples selected with the sampling design. This is the case, for instance, when estimating the mean (abolute or squared) error or purity of a given map unit from a simple random sample. The number of selected sampling units within the map unit is uncontrolled and varies between simple random samples. In this case we can either estimate the mean error or purity of a map unit $u$ using the *known* size (area, number of pixels) of map unit $u$ in the denominator, or the *estimated* size. Interestingly, by dividing by the *estimated* size of the map unit instead of its known area, the estimator generally becomes more precise [@sar92]. See also Section \@ref(LargeDomainsDirectEstimator). 

All probability sampling designs described in Part I are in principle appropriate for validation. @steh99 evaluated five basic probability sampling designs and concluded that in general stratified random sampling is a good choice. For validation of categorical maps natural strata are the map units, i.e. the groups of polygons or grid cells assigned to each class. Systematic random sampling is less suitable as no unbiased estimator of the sampling variance of the estimator of a population mean exists for this design (see Chapter \@ref(SY)). For validation of maps of extensive areas, think of whole continents, travel time between sampling locations can become substantial. In this case sampling designs that lead to spatial clustering\index{Spatial clustering} of validation locations can become efficient, for instance two-stage cluster random sampling (Chapter \@ref(Twostage)) or cluster random sampling (Chapter \@ref(Cl)).

## Real-world case study

As an illustration two soil maps of the three northern counties of Xuancheng City (Anhui province, China), both depicting soil organic matter (SOM) concentration (g kg$^{-1}$) in the topsoil, are evaluated. In Section \@ref(Ospats) the data of three samples, including the stratified random sample, were merged to estimate the parameters of a spatial model for the natural log of SOM. Here only the data of the two non-random samples are used to map SOM. The stratified simple random sample is used for validation.

Two methods are used in mapping, kriging with an external drift\index{Kriging!kriging with an external drift} (KED) and a machine learning method, random forest\index{Random forest} (RF). For mapping with RF seven covariates are used: planar curvature, profile curvature, slope, temperature, precipitation, topographic wetness index and elevation. For mapping with KED only the two most important covariates in the RF model are used: precipitation and elevation. 

```{r, echo=FALSE, eval=FALSE}
sample_grid <- read.csv("data/Xuancheng_gridsample.csv")
sample_iPSM <- read.csv("data/Xuancheng_iPSMsample.csv")
sample_train <- rbind(sample_grid,sample_iPSM)

library(ranger)
set.seed(314)
set.seed(314)
forest <- ranger(SOM_A_hori~plan.curvature+profile.curvature+slope+temperature+precipitation+twi+dem, data=sample_train, num.trees=5000, importance="impurity")

#predict at validation sample
sample_test <- read.csv("data/Xuancheng_STSIsample.csv")

res <- predict(forest, sample_test)
sample_test$RF <- res$predictions

#map SOM with RF
load(file="data/Xuancheng.RData")
res <- predict(forest, grd)
grd$RF <- res$predictions

#kriging with an external drift
library(gstat)
library(geoR)
coordinates(sample_train) <- ~X+Y
vg <- variogram(SOM_A_hori ~ dem+precipitation, data = sample_train, cutoff=10000)
vgfitOLS <- fit.variogram(vg, model = vgm(model = "Sph", psill = 30, range = 10000, nugget = 0), fit.sills=c(F,T))

sample_train <- as(sample_train,"data.frame")
dGeoR <- as.geodata(
  obj=sample_train, 
  header=TRUE, 
  coords.col=4:5, 
  data.col=3,
  data.names=NULL, 
  covar.col=c(6,7,8,9,10,11,12)
)

lmSOC_REML <- likfit(geodata = dGeoR, trend=~dem+precipitation, cov.model="spherical", ini.cov.pars=c(vgfitOLS[2,2], vgfitOLS[2,3]), nugget=vgfitOLS[1,2], lik.method="REML")
vgfitREML <- vgfitOLS
vgfitREML[1,2] <- lmSOC_REML$nugget
vgfitREML[2,2] <- lmSOC_REML$sigmasq
vgfitREML[2,3] <- lmSOC_REML$phi

#predict at validation sites
coordinates(sample_train)<-~X+Y
coordinates(sample_test)<-~X+Y
predictions  <- krige(
  SOM_A_hori ~ dem+precipitation,
  sample_train,
  newdata = sample_test,
  model = vgfitREML,
  nmax = 100
)
sample_test <- as(sample_test,"data.frame")
sample_test$KED <- predictions$var1.pred

#Map SOM with KED
coordinates(grd)<-~x1+x2
predictions  <- krige(
  SOM_A_hori ~ dem+precipitation,
  sample_train,
  newdata = grd,
  model = vgfitREML,
  nmax = 100
)
grd <- as(grd,"data.frame")
grd$KED <- predictions$var1.pred

write.csv(sample_test, file="data/Xuancheng_STSIsample.csv",row.names=F)
grd$x1 <- grd$x1/1000; grd$x2 <- grd$x2/1000
save(grd,file="data/Xuancheng.RData")
```

The two maps that are to be validated are shown in Figure \@ref(fig:validatedmaps). Note that non-soil areas (built-up, water, roads) are not predicted. The maps are quite similar. The most striking difference between the maps is the smaller range of the RF predictions: they range from 9.8 to 61.5, whereas the KED predictions range from 5.3 to 90.2.

```{r validatedmaps, echo=FALSE, out.width="100%", fig.cap="Map of SOM (g kg$\\textsuperscript{-1}$) in the topsoil of Xuancheng, obtained by kriging with an external drift (KED) and random forest (RF)."}
load(file="data/Xuancheng.RData")

df <- grd[,c("x1","x2","KED","RF")]
df_lf <- df %>% pivot_longer(cols=c("KED","RF"))

ggplot(df_lf) +
  geom_raster(mapping = aes(x = x1/1000, y = x2/1000, fill=value)) +
  scale_fill_viridis_c(name="SOM", limits=c(5,90)) + 
  scale_x_continuous(name = "Easting (km)") +
  scale_y_continuous(name = "Northing (km)") +
  facet_wrap(~ name, ncol=2, nrow=1) +
  coord_fixed()
```

The two maps are evaluated by statistical validation with a stratified simple random sample of 62 units (points). The strata are the eight units of a geological map (Figure \@ref(fig:validationsample)).

```{r validationsample, echo=FALSE, fig.cap="Stratified simple random sample for validation of the two maps of soil SOM in Xuancheng, China."}
library(terra)

rmap <- rast("data/Xuancheng_geo.tif")
grd <- as.data.frame(rmap, xy=TRUE, na.rm=TRUE)
units <- which(grd$Xuancheng_geo==99)
grd <- grd[-units,]
#1: granite and granodiorite
#2: pyroclastic rocks
#3: conglomerate
#4: sandstone
#5: limestone
#6: Quaternary siltstone, gravel and sandy clay
#7: Quaternary vermicule boulder and gravel clay
#8: shale

mysample <- read.csv(file="data/Xuancheng_STSIsample.csv", header=TRUE)

ggplot(data=grd) +
  geom_raster(mapping = aes(x = x/1000, y = y/1000,fill=factor(Xuancheng_geo))) +
  scale_fill_viridis_d(name = "Stratum") +
  scale_x_continuous(name = "Easting (km)") +
  scale_y_continuous(name = "Northing (km)") +
  geom_point(data=mysample,mapping = aes(x=X/1000,y=Y/1000),size=2) +
coord_fixed()
```


### Estimation of the population Mean Error and Mean Squared Error

To estimate the population MSE of the two maps, first the squared prediction errors are computed. The name of the measured study variable at the validation sample\index{Validation sample} in the data frame `mysample` is `SOM_A_hori`. Four new columns are added to `mysample`, by computing the prediction errors for KED and RF and squaring these errors. The next code chunks show the computations for KED only.

```{r}
mysample <- read.csv(file="data/Xuancheng_STSIsample.csv",
  header=TRUE)
mysample$eKED <- mysample$SOM_A_hori-mysample$KED
mysample$e2KED <- (mysample$eKED)^2
```

```{r, echo=FALSE}
mysample$eRF <- mysample$SOM_A_hori-mysample$RF
mysample$e2RF <- (mysample$eRF)^2
```

These four new variables now are our study variables, of which we would like to estimate the population means. These population means can be estimated as explained in Chapter \@ref(STSI). First the stratum weights are computed, that is the relative number of raster cells.

```{r}
strata <- read.csv(file="data/Xuancheng_StrataSize.csv")
w_h <- strata$Nh/sum(strata$Nh)
```

Now the stratum means of the prediction errors, obtained with KED and RF, are estimated by the sample means, and the populations mean of the errors are estimated by the weighted mean of the estimated stratum means.

```{r}
me_KED_h <- tapply(mysample$eKED, INDEX=mysample$stratum, FUN=mean)
me_KED <- sum(w_h*me_KED_h)
```

```{r, echo=FALSE}
me_RF_h <- tapply(mysample$eRF, INDEX=mysample$stratum, FUN=mean)
me_RF <- sum(w_h*me_RF_h)
```

This is repeated for the squared prediction errors.

```{r}
mse_KED_h <- tapply(mysample$e2KED, INDEX=mysample$stratum, FUN=mean)
mse_KED <- sum(w_h*mse_KED_h)
```

```{r, echo=FALSE}
mse_RF_h <- tapply(mysample$e2RF, INDEX=mysample$stratum, FUN=mean)
mse_RF <- sum(w_h*mse_RF_h)
```

#### Exercises {-}

1. The estimated MSE of the KED map equals `r as.character(round(mse_KED,1))`, that of the RF map `r as.character(round(mse_RF,1))`. Are you certain that the population MSE of the KED map is smaller than the population MSE of the RF map? 

### Estimation of the standard error of the estimated population ME and MSE

We are uncertain about both population MSEs, as we measured the squared errors at `r nrow(mysample)` sampling points only. So, we would like to know how uncertain we are. This uncertainty is quantified by the standard error of the estimated population MSE. A problem is that in the second stratum we have only one sampling point. So, for this stratum we cannot compute the variance of the squared errors. To compute the variance we need at least two sampling points. 

```{r}
table(mysample$stratum)
```

A solution is to merge stratum 2 with stratum 1, which is a similar geological map unit (we know this from the domain expert). This is referred to as collapsing the strata. A look-up table `lut` is constructed to add the collapsed strata identifiers to the data frames `mysample` and `strata`, using function `merge`.

```{r}
levels <- sort(unique(mysample$stratum))
collapsedstrata <- c(1,1,2,3,4,5,6,7)
lut <- data.frame(stratum=levels, collapsedstrata)
mysample <- merge(x=mysample, y=lut)
strata <- merge(x=strata, y=lut)
```

Now the collapsed strata\index{Collapsed strata} can be used to estimate the standard errors of the estimated population MSEs. As a first step the stratum weights and the sample sizes of the collapsed strata are computed.

```{r}
N_hc <- tapply(strata$Nh, INDEX=strata$collapsedstrata, FUN=sum)
w_hc <- N_hc/sum(N_hc)
n_hc <- table(mysample$collapsedstrata)
```

The sampling variance of the estimator of the mean of the (squared) prediction error can be estimated by Equation \@ref(eq:EstVarMeanSTSI). In the next code chunk this is shown for the KED predictions only.

```{r}
s2e_KED_hc <- tapply(mysample$eKED, INDEX=mysample$collapsedstrata, FUN=var)
se_me_KED <- sqrt(sum(w_hc^2*s2e_KED_hc/n_hc))
s2e2_KED_hc <- tapply(mysample$e2KED, INDEX=mysample$collapsedstrata, FUN=var)
se_mse_KED <- sqrt(sum(w_hc^2*s2e2_KED_hc/n_hc))
```

```{r, echo=FALSE}
s2e_RF_hc <- tapply(mysample$eRF, INDEX=mysample$collapsedstrata, FUN=var)
se_me_RF <- sqrt(sum(w_hc^2*s2e_RF_hc/n_hc))
s2e2_RF_hc <- tapply(mysample$e2RF, INDEX=mysample$collapsedstrata, FUN=var)
se_mse_RF <- sqrt(sum(w_hc^2*s2e2_RF_hc/n_hc))
```


```{r validationresults, echo=FALSE}
valres <- data.frame(KED=c(round(me_KED,2), round(mse_KED,1)), 
                     seKED=c(round(se_me_KED,2), round(se_mse_KED,1)),
                     RF=c(round(me_RF,2), round(mse_RF,1)),
                     seRF=c(round(se_me_RF,2), round(se_mse_RF,1)))
rownames(valres) <- c("ME","MSE")

knitr::kable(
  valres, caption = 'Estimated population mean error (ME) and mean squared error (MSE) of KED and RF map, and their standard errors.',
  booktabs = TRUE
) %>%
  kable_classic()
```

#### Exercises {-}

2. Do you think there is a systematic error in the KED and RF predictions?   
3. Do you think the difference between the two estimated population MSEs is statistically significant?  

### Estimation of MEC

To estimate MEC, we must first estimate the population variance of the study variable from the stratified simple random sample (the denominator in Equation \@ref(eq:HTMEC)). This population variance is estimated with function `s2` of package **surveyplanning** (Section \@ref(WhyStratify)).

```{r}
library(surveyplanning)
lut <- data.frame(collapsedstrata=1:7, weight=as.numeric(N_hc/n_hc))
mysample <- merge(x=mysample, y=lut)
s2z <- s2(mysample$SOM_A_hori, w=mysample$weight)
```

Now the MEC's for KED and RF can be estimated.

```{r}
mec_KED <- 1-(mse_KED/s2z)
mec_RF <- 1-(mse_RF/s2z)
```

The estimated MEC for KED equals `r round(mec_KED,3)` and for RF `r round(mec_RF,3)`, showing that the two models used in mapping are no better than the estimated mean of SOM used as a predictor. This is quite a disappointing result.

### Statistical testing of hypothesis about population ME and MSE

The hypothesis that the population ME equals 0 can be tested by a one-sample $t$-test\index{One-sample $t$-test}. The alternative hypothesis is that ME is unequal to 0 (two-sided alternative). The number of degrees of freedom of the $t$ distribution is approximated by the total sample size minus the number of strata (Section \@ref(CISTSI)). Note that we have a two-sided alternative hypothesis\index{Two-sided alternative hypothesis}, so we must compute a two-sided *p*-value\index{\emph{p}-value of a test!Two-sided \emph{p}-value}.

```{r}
t_KED <- me_KED/se_me_KED
df <- nrow(mysample)-length(unique(mysample$collapsedstrata))
lowertail <- (t_KED<0)
p_KED <- 2*pt(t_KED, df=df, lower.tail=lowertail)
```

```{r,echo=FALSE}
t_RF <- me_RF/se_me_RF
lowertail <- (t_RF<0)
p_RF <- 2*pt(t_RF, df=df, lower.tail=lowertail)
```

The outcomes of the test statistics are `r as.character(round(t_KED,3))` and `r as.character(round(t_RF,3))` for KED and RF, respectively, with *p*-values `r as.character(round(p_KED,3))` and `r as.character(round(p_RF,3))`. So, we clearly have not enough evidence for systematic errors, neither with KED, nor with RF mapping.

Now we test whether the two population MSEs  differ significantly. This can be done by a paired $t$-test\index{Paired $t$-test}. The first step in a paired t-test is to compute pairwise differences of squared predictions errors, and then we can proceed as in a one-sample t-test. 

```{r}
mysample$dife2 <- mysample$e2KED-mysample$e2RF
mdife2_h <- tapply(mysample$dife2, INDEX=mysample$stratum, FUN=mean)
mdife2 <- sum(w_h*mdife2_h)
vardife2_h <- tapply(mysample$dife2, INDEX=mysample$collapsedstrata, FUN=var)
sdmdife2 <- sqrt(sum(w_hc^2*vardife2_h/n_hc))
t <- mdife2/sdmdife2
lowertail <- (t<0)
p <- 2*pt(t, df=df, lower.tail=lowertail)
```

The outcome of the test statistic is `r as.character(round(t, 3))`, with a *p*-value of `r as.character(round(p,3))`, so we clearly do not have enough evidence that the population MSEs obtained with the two mapping methods are different.

```{r, echo=FALSE}
rm(list=ls())
```

